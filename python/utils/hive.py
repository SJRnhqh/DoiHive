# python/utils/hive.py
# External dependencies / å¤–éƒ¨ä¾èµ–
from rich.progress import (
    Progress,
    SpinnerColumn,
    BarColumn,
    TextColumn,
    TimeRemainingColumn,
    TimeElapsedColumn,
)
from concurrent.futures import ThreadPoolExecutor, as_completed
from urllib.parse import urljoin, urlparse
from rich.console import Console
from bs4 import BeautifulSoup
from datetime import datetime
from typing import Dict, Any
from rich.table import Table
from pathlib import Path
from rich import box
import requests
import logging
import json
import time
import re
import random


# Local modules / æœ¬åœ°æ¨¡å—
from .logger import log_to_file_only


def pdf_hive(
    urls: list[str], pdf_dir: Path, error_dir: Path = None, max_workers: int = 3
):
    """
    æ‰¹é‡ä¸‹è½½ PDF æ–‡ä»¶ï¼ˆå¤šçº¿ç¨‹ç‰ˆæœ¬ï¼‰
    Batch download PDF files (multi-threaded)

    Args:
        urls (list[str]): PDF URL åˆ—è¡¨ / List of PDF URLs
        pdf_dir (Path): è¾“å‡ºç›®å½• / Output directory
        error_dir (Path): é”™è¯¯æ—¥å¿—ç›®å½•ï¼Œå¦‚æœä¸º None åˆ™ä½¿ç”¨ pdf_dir / Error log directory
        max_workers (int): æœ€å¤§å¹¶å‘çº¿ç¨‹æ•° / Maximum number of concurrent threads
    """
    # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨ / Ensure output directory exists
    pdf_dir.mkdir(parents=True, exist_ok=True)

    # è®¾ç½®é”™è¯¯æ—¥å¿—ç›®å½•å’Œæ–‡ä»¶è·¯å¾„ / Set error log directory and file path
    if error_dir is None:
        error_dir = pdf_dir
    else:
        error_dir.mkdir(parents=True, exist_ok=True)

    # æ ¹æ®æ—¶é—´æ„é€ é”™è¯¯æ—¥å¿—æ–‡ä»¶å / Construct error log filename based on timestamp
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    error_log_filename = f"download_errors_{timestamp}.json"
    error_log_path = error_dir / error_log_filename

    # ç»Ÿè®¡ä¿¡æ¯ / Statistics
    stats = {
        "total": len(urls),
        "success": 0,
        "skip": 0,
        "failed": 0,
        "errors": [],
        "total_size": 0,  # æ€»ä¸‹è½½å¤§å° / Total download size
        "download_times": [],  # æ¯ä¸ªæ–‡ä»¶çš„ä¸‹è½½æ—¶é—´ / Download time for each file
        "success_times": [],  # æˆåŠŸä¸‹è½½çš„æ—¶é—´ / Success download times
    }

    # åˆ›å»º Rich æ§åˆ¶å° / Create Rich console
    logger = logging.getLogger("doihive")
    console = Console()
    
    info_msg = f"ğŸ“š å¼€å§‹æ‰¹é‡ä¸‹è½½ï¼Œå…± {stats['total']} ä¸ª URL"
    log_to_file_only(logging.INFO, info_msg)
    # æ§åˆ¶å°ç¾åŒ–è¾“å‡ºï¼Œä¸é‡å¤æ—¥å¿— / Beautified console output, no duplicate log
    console.print(
        f"\n[bold cyan]ğŸ“š å¼€å§‹æ‰¹é‡ä¸‹è½½[/bold cyan] [yellow]å…± {stats['total']} ä¸ª URL[/yellow]"
    )

    worker_msg = f"ğŸ”§ ä½¿ç”¨ {max_workers} ä¸ªå¹¶å‘çº¿ç¨‹"
    log_to_file_only(logging.INFO, worker_msg)
    console.print(
        f"[bold cyan]ğŸ”§ ä½¿ç”¨[/bold cyan] [yellow]{max_workers} ä¸ªå¹¶å‘çº¿ç¨‹[/yellow]\n"
    )

    # è®°å½•å¼€å§‹æ—¶é—´ / Record start time
    start_time = time.time()

    # åˆ›å»ºå¤ç”¨çš„ Sessionï¼ˆè¿æ¥æ± ä¼˜åŒ–ï¼‰/ Create reusable Session (connection pool optimization)
    session = requests.Session()
    # è®¾ç½®å®Œæ•´çš„æµè§ˆå™¨è¯·æ±‚å¤´ï¼Œé¿å…è¢«è¯†åˆ«ä¸ºçˆ¬è™« / Set complete browser headers to avoid being identified as a crawler
    session.headers.update({
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
        "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8",
        "Accept-Language": "en-US,en;q=0.9",
        "Accept-Encoding": "gzip, deflate, br",
        "Connection": "keep-alive",
        "Upgrade-Insecure-Requests": "1",
        "Sec-Fetch-Dest": "document",
        "Sec-Fetch-Mode": "navigate",
        "Sec-Fetch-Site": "none",
        "Cache-Control": "max-age=0",
    })
    # é…ç½®è¿æ¥æ±  / Configure connection pool
    adapter = requests.adapters.HTTPAdapter(
        pool_connections=max_workers * 2,  # æœ€å¤§è¿æ¥æ± æ•° / Max connection pools
        pool_maxsize=max_workers * 2,      # æ¯ä¸ªæ± çš„æœ€å¤§è¿æ¥æ•° / Max connections per pool
        max_retries=0,                     # ç¦ç”¨é‡è¯•ï¼ˆç”±å¤–éƒ¨å¤„ç†é”™è¯¯ï¼‰/ Disable retries (handle errors externally)
    )
    session.mount("http://", adapter)
    session.mount("https://", adapter)

    # åˆ›å»º Rich è¿›åº¦æ¡ / Create Rich progress bar
    progress = Progress(
        SpinnerColumn(),
        TextColumn("[progress.description]{task.description}"),
        BarColumn(),
        TextColumn("[progress.percentage]{task.percentage:>3.0f}%"),
        TextColumn("â€¢"),
        TextColumn("[green]âœ… {task.fields[success]}[/green]"),
        TextColumn("[yellow]â­ï¸ {task.fields[skip]}[/yellow]"),
        TextColumn("[red]âŒ {task.fields[failed]}[/red]"),
        TextColumn("â€¢"),
        TimeElapsedColumn(),
        TextColumn("<"),
        TimeRemainingColumn(),
        console=console,
        expand=True,
    )

    # ä½¿ç”¨çº¿ç¨‹æ± æ‰§è¡Œä¸‹è½½ä»»åŠ¡ / Use thread pool to execute download tasks
    with progress:
        task_id = progress.add_task(
            "[cyan]ğŸ“¥ ä¸‹è½½è¿›åº¦[/cyan]",
            total=stats["total"],
            success=0,
            skip=0,
            failed=0,
        )

        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            # æäº¤æ‰€æœ‰ä»»åŠ¡å¹¶è®°å½•å¼€å§‹æ—¶é—´ / Submit all tasks and record start time
            future_to_url = {}
            future_to_start_time = {}
            for url in urls:
                future = executor.submit(_download_single_pdf, url, session, pdf_dir)
                future_to_url[future] = url
                future_to_start_time[future] = time.time()

            # å¤„ç†å®Œæˆçš„ä»»åŠ¡ / Process completed tasks
            for future in as_completed(future_to_url):
                url = future_to_url[future]
                file_start_time = future_to_start_time[future]

                try:
                    result = future.result()
                    file_duration = time.time() - file_start_time
                    stats["download_times"].append(file_duration)

                    if result["status"] == "success":
                        stats["success"] += 1
                        stats["total_size"] += result["size"]
                        stats["success_times"].append(file_duration)  # è®°å½•æˆåŠŸæ—¶é—´
                    elif result["status"] == "skip":
                        stats["skip"] += 1
                    else:  # failed / å¤±è´¥
                        stats["failed"] += 1
                        error_info = {
                            "url": url,
                            "doi": result.get("doi", ""),
                            "error": result.get("error", "æœªçŸ¥é”™è¯¯"),
                            "timestamp": datetime.now().isoformat(),
                        }
                        stats["errors"].append(error_info)

                    # æ›´æ–°è¿›åº¦æ¡ / Update progress bar
                    progress.update(
                        task_id,
                        advance=1,
                        success=stats["success"],
                        skip=stats["skip"],
                        failed=stats["failed"],
                    )

                except Exception as e:
                    stats["failed"] += 1
                    file_duration = time.time() - file_start_time
                    stats["download_times"].append(file_duration)
                    error_info = {
                        "url": url,
                        "doi": "",
                        "error": f"å¼‚å¸¸: {str(e)}",
                        "timestamp": datetime.now().isoformat(),
                    }
                    stats["errors"].append(error_info)
                    progress.update(
                        task_id,
                        advance=1,
                        success=stats["success"],
                        skip=stats["skip"],
                        failed=stats["failed"],
                    )

    # è®¡ç®—æ€»æ—¶é—´å’Œå¹³å‡æ—¶é—´ / Calculate total time and average time
    total_time = time.time() - start_time
    avg_time = (
        sum(stats["download_times"]) / len(stats["download_times"])
        if stats["download_times"]
        else 0
    )

    # ä¿å­˜é”™è¯¯æ—¥å¿— / Save error log
    if stats["errors"]:
        with open(error_log_path, "w", encoding="utf-8") as f:
            json.dump(
                {
                    "summary": {
                        "total_errors": len(stats["errors"]),
                        "generated_at": datetime.now().isoformat(),
                    },
                    "errors": stats["errors"],
                },
                f,
                ensure_ascii=False,
                indent=2,
            )
        error_log_msg = f"ğŸ“ é”™è¯¯æ—¥å¿—å·²ä¿å­˜åˆ°: {error_log_path}"
        log_to_file_only(logging.WARNING, error_log_msg)

        # è®°å½•æ‰€æœ‰é”™è¯¯åˆ°æ—¥å¿—æ–‡ä»¶ / Log all errors to file
        for error in stats["errors"]:
            log_to_file_only(
                logging.ERROR,
                f"ä¸‹è½½å¤±è´¥ - DOI: {error.get('doi', 'N/A')}, URL: {error.get('url', 'N/A')}, é”™è¯¯: {error.get('error', 'N/A')}",
            )

        # æ§åˆ¶å°ç”¨ Rich è¡¨æ ¼å±•ç¤ºé”™è¯¯ / Display errors in Rich table on console
        console.print(
            f"\n[bold yellow]ğŸ“ é”™è¯¯æ—¥å¿—å·²ä¿å­˜åˆ°:[/bold yellow] [cyan]{error_log_path}[/cyan]"
        )

        # æŒ‰é”™è¯¯ç±»å‹åˆ†ç»„ç»Ÿè®¡ / Group errors by error type
        error_groups = {}
        for error in stats["errors"]:
            error_msg = error.get("error", "æœªçŸ¥é”™è¯¯")
            # æå–é”™è¯¯ç±»å‹ï¼ˆå»é™¤åŠ¨æ€éƒ¨åˆ†ï¼‰/ Extract error type (remove dynamic parts)
            error_type = error_msg
            # å¯¹äºåŒ…å«å†’å·çš„é”™è¯¯ï¼Œæå–å‰ç¼€ä½œä¸ºç±»å‹ / For errors with colons, extract prefix as type
            if ":" in error_msg:
                error_type = error_msg.split(":", 1)[0]
            
            if error_type not in error_groups:
                error_groups[error_type] = []
            error_groups[error_type].append(error)

        # åˆ›å»ºé”™è¯¯æ±‡æ€»è¡¨æ ¼ / Create error summary table
        error_table = Table(
            title=f"âŒ ä¸‹è½½å¤±è´¥æ±‡æ€» / Download Error Summary ({len(stats['errors'])} ä¸ªé”™è¯¯ï¼Œ{len(error_groups)} ç§ç±»å‹)",
            box=box.ROUNDED,
        )
        error_table.add_column(
            "é”™è¯¯ç±»å‹", style="red", no_wrap=False, width=40
        )
        error_table.add_column(
            "æ•°é‡", style="yellow", justify="right", width=8, no_wrap=True
        )
        error_table.add_column(
            "ç¤ºä¾‹ DOI", style="cyan", no_wrap=False, width=35
        )

        # æŒ‰æ•°é‡é™åºæ’åº / Sort by count in descending order
        sorted_groups = sorted(
            error_groups.items(), key=lambda x: len(x[1]), reverse=True
        )

        for error_type, errors in sorted_groups:
            count = len(errors)
            # æ”¶é›†ç¤ºä¾‹ DOIï¼ˆæœ€å¤š3ä¸ªï¼‰/ Collect example DOIs (max 3)
            example_dois = []
            for error in errors[:3]:
                doi = error.get("doi", "N/A")
                if len(doi) > 30:
                    doi = doi[:27] + "..."
                example_dois.append(doi)
            
            example_str = ", ".join(example_dois)
            if count > 3:
                example_str += f" ... (å…± {count} ä¸ª)"
            
            error_table.add_row(error_type, str(count), example_str)

        console.print()
        console.print(error_table)

    # æ ¼å¼åŒ–æ–‡ä»¶å¤§å° / Format file size
    def format_size(size_bytes):
        """æ ¼å¼åŒ–æ–‡ä»¶å¤§å° / Format file size"""
        for unit in ["B", "KB", "MB", "GB"]:
            if size_bytes < 1024.0:
                return f"{size_bytes:.2f} {unit}"
            size_bytes /= 1024.0
        return f"{size_bytes:.2f} TB"

    # æ ¼å¼åŒ–æ—¶é—´ / Format time
    def format_time(seconds):
        """æ ¼å¼åŒ–æ—¶é—´ / Format time"""
        if seconds < 60:
            return f"{seconds:.2f} ç§’"
        elif seconds < 3600:
            minutes = seconds / 60
            return f"{minutes:.2f} åˆ†é’Ÿ"
        else:
            hours = seconds / 3600
            return f"{hours:.2f} å°æ—¶"

    # åˆ›å»ºç»Ÿè®¡è¡¨æ ¼ / Create statistics table
    table = Table(
        title="ğŸ“Š ä¸‹è½½æ±‡æ€»ç»Ÿè®¡ / Download Summary Statistics", box=box.ROUNDED
    )
    table.add_column("é¡¹ç›® / Item", style="cyan", no_wrap=True)
    table.add_column("æ•°å€¼ / Value", style="magenta", justify="right")

    table.add_row("ğŸ“ æ€»è®¡ / Total", f"{stats['total']:>6} ä¸ªæ–‡ä»¶")
    table.add_row("âœ… æˆåŠŸ / Success", f"[green]{stats['success']:>6}[/green] ä¸ªæ–‡ä»¶")
    table.add_row("â­ï¸  è·³è¿‡ / Skipped", f"[yellow]{stats['skip']:>6}[/yellow] ä¸ªæ–‡ä»¶")
    table.add_row("âŒ å¤±è´¥ / Failed", f"[red]{stats['failed']:>6}[/red] ä¸ªæ–‡ä»¶")
    table.add_section()

    if stats["total"] > 0:
        success_rate = (stats["success"] / stats["total"]) * 100
        table.add_row(
            "ğŸ“ˆ æˆåŠŸç‡ / Success Rate", f"[green]{success_rate:>5.2f}%[/green]"
        )
    if stats["total_size"] > 0:
        table.add_row(
            "ğŸ’¾ æ€»å¤§å° / Total Size", f"[cyan]{format_size(stats['total_size'])}[/cyan]"
        )
    table.add_section()

    table.add_row(
        "â±ï¸  æ€»è€—æ—¶ / Total Time", f"[yellow]{format_time(total_time)}[/yellow]"
    )
    # è®¡ç®—å¹³å‡å¢™é’Ÿæ—¶é—´ï¼ˆæ€»è€—æ—¶ / æ€»ä»»åŠ¡æ•°ï¼‰/ Calculate average wall-clock time (total time / total tasks)
    if stats["total"] > 0 and total_time > 0:
        avg_wall_clock_time = total_time / stats["total"]
        table.add_row(
            "ğŸ“Š å¹³å‡å¢™é’Ÿæ—¶é—´ / Avg Wall-clock Time",
            f"[cyan]{format_time(avg_wall_clock_time)}/ä»»åŠ¡[/cyan]",
        )
    if stats["download_times"]:
        table.add_row(
            "âš¡ å¹³å‡è€—æ—¶ / Avg Time", f"[yellow]{format_time(avg_time)}[/yellow]"
        )
        if stats["success_times"]:
            avg_success_time = sum(stats["success_times"]) / len(stats["success_times"])
            table.add_row(
                "ğŸš€ æˆåŠŸå¹³å‡ / Success Avg",
                f"[green]{format_time(avg_success_time)}[/green]",
            )

    # è®°å½•ç»Ÿè®¡ä¿¡æ¯åˆ°æ—¥å¿—ï¼ˆåªå†™å…¥æ–‡ä»¶ï¼Œä¸æ˜¾ç¤ºåœ¨æ§åˆ¶å°ï¼Œé¿å…ä¸è¡¨æ ¼é‡å¤ï¼‰ / Log statistics to file only (not shown in console to avoid duplication with table)
    log_to_file_only(logging.INFO, "=" * 70)
    log_to_file_only(logging.INFO, "ğŸ“Š ä¸‹è½½æ±‡æ€»ç»Ÿè®¡:")
    log_to_file_only(logging.INFO, f"ğŸ“ æ€»è®¡: {stats['total']} ä¸ªæ–‡ä»¶")
    log_to_file_only(logging.INFO, f"âœ… æˆåŠŸ: {stats['success']} ä¸ªæ–‡ä»¶")
    log_to_file_only(logging.INFO, f"â­ï¸  è·³è¿‡: {stats['skip']} ä¸ªæ–‡ä»¶")
    log_to_file_only(logging.INFO, f"âŒ å¤±è´¥: {stats['failed']} ä¸ªæ–‡ä»¶")
    if stats["total"] > 0:
        success_rate = (stats["success"] / stats["total"]) * 100
        log_to_file_only(logging.INFO, f"ğŸ“ˆ æˆåŠŸç‡: {success_rate:.2f}%")
    if stats["total_size"] > 0:
        log_to_file_only(logging.INFO, f"ğŸ’¾ æ€»å¤§å°: {format_size(stats['total_size'])}")
    log_to_file_only(logging.INFO, f"â±ï¸  æ€»è€—æ—¶: {format_time(total_time)}")
    if stats["total"] > 0 and total_time > 0:
        avg_wall_clock_time = total_time / stats["total"]
        log_to_file_only(
            logging.INFO, f"ğŸ“Š å¹³å‡å¢™é’Ÿæ—¶é—´: {format_time(avg_wall_clock_time)}/ä»»åŠ¡"
        )
    if stats["download_times"]:
        log_to_file_only(logging.INFO, f"âš¡ å¹³å‡è€—æ—¶: {format_time(avg_time)}")
        if stats["success_times"]:
            avg_success_time = sum(stats["success_times"]) / len(stats["success_times"])
            log_to_file_only(
                logging.INFO, f"ğŸš€ æˆåŠŸå¹³å‡: {format_time(avg_success_time)}"
            )
    log_to_file_only(logging.INFO, "=" * 70)

    # æ§åˆ¶å°åªæ˜¾ç¤ºè¡¨æ ¼ï¼Œä¸æ˜¾ç¤ºæ—¥å¿— / Console only shows table, no log output
    console.print()
    console.print(table)

    return stats


def _download_single_pdf(url: str, session: requests.Session, pdf_dir: Path) -> Dict[str, Any]:
    """
    ä¸‹è½½å•ä¸ª PDF æ–‡ä»¶çš„å®Œæ•´é€»è¾‘
    Complete logic for downloading a single PDF file

    Args:
        url (str): Sci-Hub é¡µé¢ URL / Sci-Hub page URL
        session (requests.Session): å¤ç”¨çš„ HTTP Sessionï¼ˆè¿æ¥æ± ï¼‰/ Reusable HTTP Session (connection pool)
        pdf_dir (Path): PDF ä¿å­˜ç›®å½• / PDF save directory

    Returns:
        dict: åŒ…å« status, filename, size, doi, error ç­‰å­—æ®µçš„å­—å…¸ /Dictionary containing status, filename, size, doi, error fields
    """
    result = {"status": "failed", "filename": "", "size": 0, "doi": "", "error": ""}

    # ä» URL ä¸­æå– DOI / Extract DOI from URL
    parsed_url = urlparse(url)
    doi = parsed_url.path.lstrip("/")
    result["doi"] = doi

    # æ¸…ç† DOI ä¸­çš„ç‰¹æ®Šå­—ç¬¦ï¼Œç”¨äºæ–‡ä»¶å / Clean special characters in DOI for filename
    safe_filename = doi.replace("/", "_").replace(":", "_")
    pdf_filename = f"{safe_filename}.pdf"
    result["filename"] = pdf_filename
    pdf_file_path = pdf_dir / pdf_filename

    # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å·²å­˜åœ¨ / Check if file already exists
    if pdf_file_path.exists():
        result["status"] = "skip"
        result["size"] = pdf_file_path.stat().st_size
        return result

    # ç¬¬ä¸€æ­¥ï¼šè·å–é¡µé¢ HTML / Step 1: Get page HTML
    # æ·»åŠ éšæœºå»¶è¿Ÿï¼Œé¿å…è¯·æ±‚è¿‡å¿«è¢«è¯†åˆ«ä¸ºçˆ¬è™« / Add random delay to avoid being identified as a crawler
    time.sleep(random.uniform(0.5, 2.0))
    
    # é‡è¯•æœºåˆ¶ï¼šæœ€å¤šé‡è¯• 3 æ¬¡ / Retry mechanism: up to 3 retries
    max_retries = 3
    retry_delay = 2  # åˆå§‹é‡è¯•å»¶è¿Ÿï¼ˆç§’ï¼‰/ Initial retry delay (seconds)
    
    for attempt in range(max_retries):
        try:
            response = session.get(url, timeout=10)
            
            # å¦‚æœæ˜¯ 403 é”™è¯¯ï¼Œç­‰å¾…åé‡è¯• / If 403 error, wait and retry
            if response.status_code == 403:
                if attempt < max_retries - 1:
                    wait_time = retry_delay * (attempt + 1) + random.uniform(0, 2)
                    time.sleep(wait_time)
                    continue
                else:
                    result["error"] = f"é¡µé¢è¯·æ±‚å¤±è´¥: HTTP 403 (å·²é‡è¯• {max_retries} æ¬¡)"
                    return result
            
            response.raise_for_status()
            break  # æˆåŠŸï¼Œé€€å‡ºé‡è¯•å¾ªç¯ / Success, exit retry loop
            
        except requests.exceptions.RequestException as e:
            if attempt < max_retries - 1:
                wait_time = retry_delay * (attempt + 1) + random.uniform(0, 2)
                time.sleep(wait_time)
                continue
            else:
                result["error"] = f"é¡µé¢è¯·æ±‚å¤±è´¥: {str(e)} (å·²é‡è¯• {max_retries} æ¬¡)"
                return result
    else:
        # æ‰€æœ‰é‡è¯•éƒ½å¤±è´¥äº† / All retries failed
        result["error"] = f"é¡µé¢è¯·æ±‚å¤±è´¥: å·²é‡è¯• {max_retries} æ¬¡"
        return result

    html_content = response.text
    soup = BeautifulSoup(html_content, "html.parser")

    # ç¬¬äºŒæ­¥ï¼šæå– PDF URL / Step 2: Extract PDF URL
    pdf_url = _extract_pdf_url(html_content, soup, url)

    if not pdf_url:
        result["error"] = "æœªèƒ½ä»é¡µé¢ä¸­æå– PDF URL"
        return result

    # ç¬¬ä¸‰æ­¥ï¼šä¸‹è½½ PDF æ–‡ä»¶ / Step 3: Download PDF file
    # æ·»åŠ éšæœºå»¶è¿Ÿ / Add random delay
    time.sleep(random.uniform(0.3, 1.0))
    
    # ä¸º PDF ä¸‹è½½æ·»åŠ  Referer å¤´ / Add Referer header for PDF download
    headers_for_pdf = {"Referer": url}
    
    # é‡è¯•æœºåˆ¶ï¼šæœ€å¤šé‡è¯• 3 æ¬¡ / Retry mechanism: up to 3 retries
    for attempt in range(max_retries):
        try:
            pdf_response = session.get(pdf_url, timeout=30, stream=True, headers=headers_for_pdf)
            
            # å¦‚æœæ˜¯ 403 é”™è¯¯ï¼Œç­‰å¾…åé‡è¯• / If 403 error, wait and retry
            if pdf_response.status_code == 403:
                if attempt < max_retries - 1:
                    wait_time = retry_delay * (attempt + 1) + random.uniform(0, 2)
                    time.sleep(wait_time)
                    continue
                else:
                    result["error"] = f"PDF ä¸‹è½½å¤±è´¥: HTTP 403 (å·²é‡è¯• {max_retries} æ¬¡)"
                    return result
            
            pdf_response.raise_for_status()
            break  # æˆåŠŸï¼Œé€€å‡ºé‡è¯•å¾ªç¯ / Success, exit retry loop
            
        except requests.exceptions.RequestException as e:
            if attempt < max_retries - 1:
                wait_time = retry_delay * (attempt + 1) + random.uniform(0, 2)
                time.sleep(wait_time)
                continue
            else:
                result["error"] = f"PDF ä¸‹è½½å¤±è´¥: {str(e)} (å·²é‡è¯• {max_retries} æ¬¡)"
                if pdf_file_path.exists():
                    pdf_file_path.unlink()
                return result
    else:
        # æ‰€æœ‰é‡è¯•éƒ½å¤±è´¥äº† / All retries failed
        result["error"] = f"PDF ä¸‹è½½å¤±è´¥: å·²é‡è¯• {max_retries} æ¬¡"
        if pdf_file_path.exists():
            pdf_file_path.unlink()
        return result

    # ä½¿ç”¨ stream=True ä¸‹è½½å¤§æ–‡ä»¶ / Use stream=True to download large files
    try:
        with open(pdf_file_path, "wb") as f:
            for chunk in pdf_response.iter_content(chunk_size=8192):
                if chunk:
                    f.write(chunk)

        # æ£€æŸ¥æ–‡ä»¶å¤§å° / Check file size
        file_size = pdf_file_path.stat().st_size
        if file_size == 0:
            result["error"] = "ä¸‹è½½çš„æ–‡ä»¶å¤§å°ä¸º 0"
            pdf_file_path.unlink()
            return result

        # éªŒè¯æ–‡ä»¶æ˜¯å¦ä¸ºæœ‰æ•ˆçš„ PDFï¼ˆæ£€æŸ¥æ–‡ä»¶å¤´ï¼‰/ Validate if file is a valid PDF (check file header)
        with open(pdf_file_path, "rb") as f:
            file_header = f.read(4)
            if file_header != b"%PDF":
                result["error"] = "ä¸‹è½½çš„æ–‡ä»¶ä¸æ˜¯æœ‰æ•ˆçš„ PDF æ–‡ä»¶"
                pdf_file_path.unlink()
                return result

        result["status"] = "success"
        result["size"] = file_size
        return result

    except Exception as e:
        result["error"] = f"æ–‡ä»¶å†™å…¥å¤±è´¥: {str(e)}"
        if pdf_file_path.exists():
            pdf_file_path.unlink()
        return result


def _extract_pdf_url(
    html_content: str, soup: BeautifulSoup, base_url: str
) -> str | None:
    """
    ä» HTML ä¸­æå– PDF URL
    Extract PDF URL from HTML

    Args:
        html_content (str): HTML å†…å®¹å­—ç¬¦ä¸² / HTML content string
        soup (BeautifulSoup): BeautifulSoup è§£æå¯¹è±¡ / BeautifulSoup parsed object
        base_url (str): åŸºç¡€ URL / Base URL

    Returns:
        str | None: PDF URL æˆ– None / PDF URL or None
    """
    pdf_url = None

    # æ–¹æ³•1ï¼šä¼˜å…ˆä½¿ç”¨ BeautifulSoup æŸ¥æ‰¾ä¸‹è½½é“¾æ¥ / Method 1: Use BeautifulSoup to find download link (preferred)
    download_div = soup.find("div", class_="download")
    if download_div:
        a_tag = download_div.find("a")
        if a_tag and a_tag.get("href"):
            download_path = a_tag.get("href")
            pdf_url = urljoin(base_url, download_path)
            return pdf_url

    # æ–¹æ³•2ï¼šä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æå–ä¸‹è½½é“¾æ¥ï¼ˆå¤‡ç”¨æ–¹æ¡ˆï¼‰/ Method 2: Use regex to extract download link (fallback)
    pattern = r'<div[^>]*class\s*=\s*["\']download["\'][^>]*>.*?<a[^>]+href\s*=\s*["\']([^"\']+)["\']'
    match = re.search(pattern, html_content, re.IGNORECASE | re.DOTALL)
    if match:
        download_path = match.group(1)
        pdf_url = urljoin(base_url, download_path)
        return pdf_url

    # æ–¹æ³•3ï¼šå¦‚æœä¸‹è½½é“¾æ¥ä¸å­˜åœ¨ï¼Œå†ä½¿ç”¨ object æ ‡ç­¾ï¼ˆå¤‡ç”¨æ–¹æ¡ˆï¼‰/ Method 3: Use object tag if download link not found (fallback)
    object_tag = soup.find("object", type="application/pdf")
    if not object_tag:
        object_tag = soup.find("object", attrs={"data": True})

    if object_tag and object_tag.get("data"):
        pdf_path = object_tag.get("data")
        if "#" in pdf_path:
            pdf_path = pdf_path.split("#")[0]
        pdf_url = urljoin(base_url, pdf_path)
        return pdf_url

    # æ–¹æ³•4ï¼šä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æå– object æ ‡ç­¾çš„ data å±æ€§ï¼ˆæœ€åå¤‡ç”¨æ–¹æ¡ˆï¼‰/ Method 4: Use regex to extract object tag data attribute (last fallback)
    pattern = r'<object[^>]+data\s*=\s*["\']([^"\']+)["\']'
    match = re.search(pattern, html_content, re.IGNORECASE)
    if match:
        pdf_path = match.group(1)
        if "#" in pdf_path:
            pdf_path = pdf_path.split("#")[0]
        pdf_url = urljoin(base_url, pdf_path)
        return pdf_url

    return None
