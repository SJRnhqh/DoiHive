# python/utils/hive.py
# External dependencies / å¤–éƒ¨ä¾èµ–
from rich.progress import Progress, SpinnerColumn, BarColumn, TextColumn, TimeRemainingColumn, TimeElapsedColumn
from concurrent.futures import ThreadPoolExecutor, as_completed
from urllib.parse import urljoin, urlparse
from rich.console import Console
from bs4 import BeautifulSoup
from datetime import datetime
from typing import Dict, Any
from rich.table import Table
from pathlib import Path
from rich import box
import requests
import json
import time
import re


def pdf_hive(
    urls: list[str], pdf_dir: Path, error_dir: Path = None, max_workers: int = 4
):
    """
    æ‰¹é‡ä¸‹è½½ PDF æ–‡ä»¶ï¼ˆå¤šçº¿ç¨‹ç‰ˆæœ¬ï¼‰
    Batch download PDF files (multi-threaded)

    Args:
        urls (list[str]): PDF URL åˆ—è¡¨ / List of PDF URLs
        pdf_dir (Path): è¾“å‡ºç›®å½• / Output directory
        error_dir (Path): é”™è¯¯æ—¥å¿—ç›®å½•ï¼Œå¦‚æœä¸º None åˆ™ä½¿ç”¨ pdf_dir / Error log directory
        max_workers (int): æœ€å¤§å¹¶å‘çº¿ç¨‹æ•° / Maximum number of concurrent threads
    """
    # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨ / Ensure output directory exists
    pdf_dir.mkdir(parents=True, exist_ok=True)

    # è®¾ç½®é”™è¯¯æ—¥å¿—ç›®å½•å’Œæ–‡ä»¶è·¯å¾„ / Set error log directory and file path
    if error_dir is None:
        error_dir = pdf_dir
    else:
        error_dir.mkdir(parents=True, exist_ok=True)

    # æ ¹æ®æ—¶é—´æ„é€ é”™è¯¯æ—¥å¿—æ–‡ä»¶å / Construct error log filename based on timestamp
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    error_log_filename = f"download_errors_{timestamp}.json"
    error_log_path = error_dir / error_log_filename

    # ç»Ÿè®¡ä¿¡æ¯ / Statistics
    stats = {
        "total": len(urls),
        "success": 0,
        "skip": 0,
        "failed": 0,
        "errors": [],
        "total_size": 0,  # æ€»ä¸‹è½½å¤§å° / Total download size
        "download_times": [],  # æ¯ä¸ªæ–‡ä»¶çš„ä¸‹è½½æ—¶é—´ / Download time for each file
        "success_times": [],  # æˆåŠŸä¸‹è½½çš„æ—¶é—´ / Success download times
    }

    # åˆ›å»º Rich æ§åˆ¶å° / Create Rich console
    console = Console()
    
    console.print(f"\n[bold cyan]ğŸ“š å¼€å§‹æ‰¹é‡ä¸‹è½½[/bold cyan] [yellow]å…± {stats['total']} ä¸ª URL[/yellow]")
    console.print(f"[bold cyan]ğŸ”§ ä½¿ç”¨[/bold cyan] [yellow]{max_workers} ä¸ªå¹¶å‘çº¿ç¨‹[/yellow]\n")

    # è®°å½•å¼€å§‹æ—¶é—´ / Record start time
    start_time = time.time()

    # è®¾ç½®è¯·æ±‚å¤´ï¼Œæ¨¡æ‹Ÿæµè§ˆå™¨ / Set request headers to simulate browser
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"
    }

    # åˆ›å»º Rich è¿›åº¦æ¡ / Create Rich progress bar
    progress = Progress(
        SpinnerColumn(),
        TextColumn("[progress.description]{task.description}"),
        BarColumn(),
        TextColumn("[progress.percentage]{task.percentage:>3.0f}%"),
        TextColumn("â€¢"),
        TextColumn("[green]âœ… {task.fields[success]}[/green]"),
        TextColumn("[yellow]â­ï¸ {task.fields[skip]}[/yellow]"),
        TextColumn("[red]âŒ {task.fields[failed]}[/red]"),
        TextColumn("â€¢"),
        TimeElapsedColumn(),
        TextColumn("<"),
        TimeRemainingColumn(),
        console=console,
        expand=True,
    )

    # ä½¿ç”¨çº¿ç¨‹æ± æ‰§è¡Œä¸‹è½½ä»»åŠ¡ / Use thread pool to execute download tasks
    with progress:
        task_id = progress.add_task(
            "[cyan]ğŸ“¥ ä¸‹è½½è¿›åº¦[/cyan]",
            total=stats["total"],
            success=0,
            skip=0,
            failed=0,
        )

        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            # æäº¤æ‰€æœ‰ä»»åŠ¡å¹¶è®°å½•å¼€å§‹æ—¶é—´ / Submit all tasks and record start time
            future_to_url = {}
            future_to_start_time = {}
            for url in urls:
                future = executor.submit(_download_single_pdf, url, headers, pdf_dir)
                future_to_url[future] = url
                future_to_start_time[future] = time.time()

            # å¤„ç†å®Œæˆçš„ä»»åŠ¡ / Process completed tasks
            for future in as_completed(future_to_url):
                url = future_to_url[future]
                file_start_time = future_to_start_time[future]

                try:
                    result = future.result()
                    file_duration = time.time() - file_start_time
                    stats["download_times"].append(file_duration)

                    if result["status"] == "success":
                        stats["success"] += 1
                        stats["total_size"] += result["size"]
                        stats["success_times"].append(file_duration)  # è®°å½•æˆåŠŸæ—¶é—´
                    elif result["status"] == "skip":
                        stats["skip"] += 1
                    else:  # failed / å¤±è´¥
                        stats["failed"] += 1
                        error_info = {
                            "url": url,
                            "doi": result.get("doi", ""),
                            "error": result.get("error", "æœªçŸ¥é”™è¯¯"),
                            "timestamp": datetime.now().isoformat(),
                        }
                        stats["errors"].append(error_info)

                    # æ›´æ–°è¿›åº¦æ¡ / Update progress bar
                    progress.update(
                        task_id,
                        advance=1,
                        success=stats["success"],
                        skip=stats["skip"],
                        failed=stats["failed"],
                    )

                except Exception as e:
                    stats["failed"] += 1
                    file_duration = time.time() - file_start_time
                    stats["download_times"].append(file_duration)
                    error_info = {
                        "url": url,
                        "doi": "",
                        "error": f"å¼‚å¸¸: {str(e)}",
                        "timestamp": datetime.now().isoformat(),
                    }
                    stats["errors"].append(error_info)
                    progress.update(
                        task_id,
                        advance=1,
                        success=stats["success"],
                        skip=stats["skip"],
                        failed=stats["failed"],
                    )

    # è®¡ç®—æ€»æ—¶é—´å’Œå¹³å‡æ—¶é—´ / Calculate total time and average time
    total_time = time.time() - start_time
    avg_time = (
        sum(stats["download_times"]) / len(stats["download_times"])
        if stats["download_times"]
        else 0
    )

    # ä¿å­˜é”™è¯¯æ—¥å¿— / Save error log
    if stats["errors"]:
        with open(error_log_path, "w", encoding="utf-8") as f:
            json.dump(
                {
                    "summary": {
                        "total_errors": len(stats["errors"]),
                        "generated_at": datetime.now().isoformat(),
                    },
                    "errors": stats["errors"],
                },
                f,
                ensure_ascii=False,
                indent=2,
            )
        console.print(f"\n[bold yellow]ğŸ“ é”™è¯¯æ—¥å¿—å·²ä¿å­˜åˆ°:[/bold yellow] [cyan]{error_log_path}[/cyan]")

    # æ ¼å¼åŒ–æ–‡ä»¶å¤§å° / Format file size
    def format_size(size_bytes):
        """æ ¼å¼åŒ–æ–‡ä»¶å¤§å° / Format file size"""
        for unit in ["B", "KB", "MB", "GB"]:
            if size_bytes < 1024.0:
                return f"{size_bytes:.2f} {unit}"
            size_bytes /= 1024.0
        return f"{size_bytes:.2f} TB"

    # æ ¼å¼åŒ–æ—¶é—´ / Format time
    def format_time(seconds):
        """æ ¼å¼åŒ–æ—¶é—´ / Format time"""
        if seconds < 60:
            return f"{seconds:.2f} ç§’"
        elif seconds < 3600:
            minutes = seconds / 60
            return f"{minutes:.2f} åˆ†é’Ÿ"
        else:
            hours = seconds / 3600
            return f"{hours:.2f} å°æ—¶"

    # åˆ›å»ºç»Ÿè®¡è¡¨æ ¼ / Create statistics table
    table = Table(title="ğŸ“Š ä¸‹è½½æ±‡æ€»ç»Ÿè®¡ / Download Summary Statistics", box=box.ROUNDED)
    table.add_column("é¡¹ç›® / Item", style="cyan", no_wrap=True)
    table.add_column("æ•°å€¼ / Value", style="magenta", justify="right")
    
    table.add_row("ğŸ“ æ€»è®¡ / Total", f"{stats['total']:>6} ä¸ªæ–‡ä»¶")
    table.add_row("âœ… æˆåŠŸ / Success", f"[green]{stats['success']:>6}[/green] ä¸ªæ–‡ä»¶")
    table.add_row("â­ï¸  è·³è¿‡ / Skipped", f"[yellow]{stats['skip']:>6}[/yellow] ä¸ªæ–‡ä»¶")
    table.add_row("âŒ å¤±è´¥ / Failed", f"[red]{stats['failed']:>6}[/red] ä¸ªæ–‡ä»¶")
    table.add_section()
    
    if stats["total"] > 0:
        success_rate = (stats["success"] / stats["total"]) * 100
        table.add_row("ğŸ“ˆ æˆåŠŸç‡ / Success Rate", f"[green]{success_rate:>5.2f}%[/green]")
    if stats["total_size"] > 0:
        table.add_row("ğŸ’¾ æ€»å¤§å° / Total Size", f"[cyan]{format_size(stats['total_size'])}[/cyan]")
    table.add_section()
    
    table.add_row("â±ï¸  æ€»è€—æ—¶ / Total Time", f"[yellow]{format_time(total_time)}[/yellow]")
    if stats["download_times"]:
        table.add_row("âš¡ å¹³å‡è€—æ—¶ / Avg Time", f"[yellow]{format_time(avg_time)}[/yellow]")
        if stats["success_times"]:
            avg_success_time = sum(stats["success_times"]) / len(stats["success_times"])
            table.add_row("ğŸš€ æˆåŠŸå¹³å‡ / Success Avg", f"[green]{format_time(avg_success_time)}[/green]")
    
    console.print()
    console.print(table)

    return stats


def _download_single_pdf(url: str, headers: dict, pdf_dir: Path) -> Dict[str, Any]:
    """
    ä¸‹è½½å•ä¸ª PDF æ–‡ä»¶çš„å®Œæ•´é€»è¾‘
    Complete logic for downloading a single PDF file

    Args:
        url (str): Sci-Hub é¡µé¢ URL / Sci-Hub page URL
        headers (dict): HTTP è¯·æ±‚å¤´ / HTTP request headers
        pdf_dir (Path): PDF ä¿å­˜ç›®å½• / PDF save directory

    Returns:
        dict: åŒ…å« status, filename, size, doi, error ç­‰å­—æ®µçš„å­—å…¸ /Dictionary containing status, filename, size, doi, error fields
    """
    result = {"status": "failed", "filename": "", "size": 0, "doi": "", "error": ""}

    # ä» URL ä¸­æå– DOI / Extract DOI from URL
    parsed_url = urlparse(url)
    doi = parsed_url.path.lstrip("/")
    result["doi"] = doi

    # æ¸…ç† DOI ä¸­çš„ç‰¹æ®Šå­—ç¬¦ï¼Œç”¨äºæ–‡ä»¶å / Clean special characters in DOI for filename
    safe_filename = doi.replace("/", "_").replace(":", "_")
    pdf_filename = f"{safe_filename}.pdf"
    result["filename"] = pdf_filename
    pdf_file_path = pdf_dir / pdf_filename

    # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å·²å­˜åœ¨ / Check if file already exists
    if pdf_file_path.exists():
        result["status"] = "skip"
        result["size"] = pdf_file_path.stat().st_size
        return result

    # ç¬¬ä¸€æ­¥ï¼šè·å–é¡µé¢ HTML / Step 1: Get page HTML
    try:
        response = requests.get(url, headers=headers, timeout=10)
        response.raise_for_status()
    except requests.exceptions.RequestException as e:
        result["error"] = f"é¡µé¢è¯·æ±‚å¤±è´¥: {str(e)}"
        return result

    html_content = response.text
    soup = BeautifulSoup(html_content, "html.parser")

    # ç¬¬äºŒæ­¥ï¼šæå– PDF URL / Step 2: Extract PDF URL
    pdf_url = _extract_pdf_url(html_content, soup, url)

    if not pdf_url:
        result["error"] = "æœªèƒ½ä»é¡µé¢ä¸­æå– PDF URL"
        return result

    # ç¬¬ä¸‰æ­¥ï¼šä¸‹è½½ PDF æ–‡ä»¶ / Step 3: Download PDF file
    try:
        pdf_response = requests.get(pdf_url, headers=headers, timeout=30, stream=True)
        pdf_response.raise_for_status()

        # ä½¿ç”¨ stream=True ä¸‹è½½å¤§æ–‡ä»¶ / Use stream=True to download large files
        with open(pdf_file_path, "wb") as f:
            for chunk in pdf_response.iter_content(chunk_size=8192):
                if chunk:
                    f.write(chunk)

        # æ£€æŸ¥æ–‡ä»¶å¤§å° / Check file size
        file_size = pdf_file_path.stat().st_size
        if file_size == 0:
            result["error"] = "ä¸‹è½½çš„æ–‡ä»¶å¤§å°ä¸º 0"
            pdf_file_path.unlink()
            return result

        # éªŒè¯æ–‡ä»¶æ˜¯å¦ä¸ºæœ‰æ•ˆçš„ PDFï¼ˆæ£€æŸ¥æ–‡ä»¶å¤´ï¼‰/ Validate if file is a valid PDF (check file header)
        with open(pdf_file_path, "rb") as f:
            file_header = f.read(4)
            if file_header != b"%PDF":
                result["error"] = "ä¸‹è½½çš„æ–‡ä»¶ä¸æ˜¯æœ‰æ•ˆçš„ PDF æ–‡ä»¶"
                pdf_file_path.unlink()
                return result

        result["status"] = "success"
        result["size"] = file_size
        return result

    except requests.exceptions.RequestException as e:
        result["error"] = f"PDF ä¸‹è½½å¤±è´¥: {str(e)}"
        if pdf_file_path.exists():
            pdf_file_path.unlink()
        return result
    except Exception as e:
        result["error"] = f"æœªçŸ¥é”™è¯¯: {str(e)}"
        if pdf_file_path.exists():
            pdf_file_path.unlink()
        return result


def _extract_pdf_url(
    html_content: str, soup: BeautifulSoup, base_url: str
) -> str | None:
    """
    ä» HTML ä¸­æå– PDF URL
    Extract PDF URL from HTML

    Args:
        html_content (str): HTML å†…å®¹å­—ç¬¦ä¸² / HTML content string
        soup (BeautifulSoup): BeautifulSoup è§£æå¯¹è±¡ / BeautifulSoup parsed object
        base_url (str): åŸºç¡€ URL / Base URL

    Returns:
        str | None: PDF URL æˆ– None / PDF URL or None
    """
    pdf_url = None

    # æ–¹æ³•1ï¼šä¼˜å…ˆä½¿ç”¨ BeautifulSoup æŸ¥æ‰¾ä¸‹è½½é“¾æ¥ / Method 1: Use BeautifulSoup to find download link (preferred)
    download_div = soup.find("div", class_="download")
    if download_div:
        a_tag = download_div.find("a")
        if a_tag and a_tag.get("href"):
            download_path = a_tag.get("href")
            pdf_url = urljoin(base_url, download_path)
            return pdf_url

    # æ–¹æ³•2ï¼šä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æå–ä¸‹è½½é“¾æ¥ï¼ˆå¤‡ç”¨æ–¹æ¡ˆï¼‰/ Method 2: Use regex to extract download link (fallback)
    pattern = r'<div[^>]*class\s*=\s*["\']download["\'][^>]*>.*?<a[^>]+href\s*=\s*["\']([^"\']+)["\']'
    match = re.search(pattern, html_content, re.IGNORECASE | re.DOTALL)
    if match:
        download_path = match.group(1)
        pdf_url = urljoin(base_url, download_path)
        return pdf_url

    # æ–¹æ³•3ï¼šå¦‚æœä¸‹è½½é“¾æ¥ä¸å­˜åœ¨ï¼Œå†ä½¿ç”¨ object æ ‡ç­¾ï¼ˆå¤‡ç”¨æ–¹æ¡ˆï¼‰/ Method 3: Use object tag if download link not found (fallback)
    object_tag = soup.find("object", type="application/pdf")
    if not object_tag:
        object_tag = soup.find("object", attrs={"data": True})

    if object_tag and object_tag.get("data"):
        pdf_path = object_tag.get("data")
        if "#" in pdf_path:
            pdf_path = pdf_path.split("#")[0]
        pdf_url = urljoin(base_url, pdf_path)
        return pdf_url

    # æ–¹æ³•4ï¼šä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æå– object æ ‡ç­¾çš„ data å±æ€§ï¼ˆæœ€åå¤‡ç”¨æ–¹æ¡ˆï¼‰/ Method 4: Use regex to extract object tag data attribute (last fallback)
    pattern = r'<object[^>]+data\s*=\s*["\']([^"\']+)["\']'
    match = re.search(pattern, html_content, re.IGNORECASE)
    if match:
        pdf_path = match.group(1)
        if "#" in pdf_path:
            pdf_path = pdf_path.split("#")[0]
        pdf_url = urljoin(base_url, pdf_path)
        return pdf_url

    return None
