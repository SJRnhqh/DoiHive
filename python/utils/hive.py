# python/utils/hive.py
# External dependencies / å¤–éƒ¨ä¾èµ–
import re
import time
from pathlib import Path
from urllib.parse import unquote, urljoin, urlparse
import requests


def pdf_hive(urls: list[str], pdf_dir: Path):
    """
    æ‰¹é‡ä¸‹è½½ PDF æ–‡ä»¶
    Batch download PDF files

    Args:
        urls (list[str]): PDF URL åˆ—è¡¨ / List of PDF URLs
        pdf_dir (Path): è¾“å‡ºç›®å½• / Output directory
    """
    pdf_dir.mkdir(parents=True, exist_ok=True)

    stats = {"total": len(urls), "success": 0, "failed": 0, "skipped": 0}

    print(f"ğŸ“¥ å¼€å§‹æ‰¹é‡ä¸‹è½½ {len(urls)} ä¸ª PDF...\n")

    for i, url in enumerate(urls, 1):
        # ä» URL æå– DOI ä½œä¸ºæ–‡ä»¶å / Extract DOI from URL as filename
        doi = url.split("/")[-1]
        filename = unquote(doi).replace("/", "_").replace(":", "_")[:200]
        output_path = pdf_dir / f"{filename}.pdf"

        # è·³è¿‡å·²å­˜åœ¨çš„æ–‡ä»¶ / Skip existing files
        if output_path.exists():
            print(f"[{i}/{len(urls)}] â­ï¸  è·³è¿‡ï¼ˆå·²å­˜åœ¨ï¼‰: {doi}")
            stats["skipped"] += 1
            continue

        print(f"[{i}/{len(urls)}] ğŸ“„ ä¸‹è½½: {doi}")

        try:
            headers = {"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"}
            response = requests.get(url, headers=headers, timeout=30)

            if response.status_code != 200:
                stats["failed"] += 1
                print(f"   âŒ HTTP {response.status_code}")
                continue

            # æ£€æŸ¥å†…å®¹ç±»å‹ / Check content type
            content_type = response.headers.get("Content-Type", "").lower()
            
            # å¦‚æœæ˜¯ PDFï¼Œç›´æ¥ä¿å­˜ / If it's PDF, save directly
            if "application/pdf" in content_type or response.content[:4] == b"%PDF":
                with open(output_path, "wb") as f:
                    f.write(response.content)
                stats["success"] += 1
                print(f"   âœ… æˆåŠŸ: {output_path.name}")
            # å¦‚æœæ˜¯ HTMLï¼Œå°è¯•æå– PDF é“¾æ¥ / If HTML, try to extract PDF link
            elif "text/html" in content_type:
                html = response.text
                
                # æ£€æŸ¥æ˜¯å¦æ˜¯é”™è¯¯é¡µé¢ / Check if it's an error page
                if "not available" in html.lower() or "article is not available" in html.lower():
                    stats["failed"] += 1
                    print(f"   âŒ æ–‡ç« ä¸å¯ç”¨ / Article not available")
                    continue
                
                # å°è¯•ä» HTML ä¸­æå– PDF URL / Try to extract PDF URL from HTML
                pdf_url = None
                
                # æ–¹æ³•1: æŸ¥æ‰¾ iframe src / Method 1: Find iframe src
                iframe_patterns = [
                    r'<iframe[^>]+src=["\']([^"\']+)["\']',
                    r'iframe\.src\s*=\s*["\']([^"\']+)["\']',
                ]
                for pattern in iframe_patterns:
                    iframe_match = re.search(pattern, html, re.IGNORECASE)
                    if iframe_match:
                        pdf_url = iframe_match.group(1)
                        # å¦‚æœæ˜¯ç›¸å¯¹è·¯å¾„ï¼Œè½¬æ¢ä¸ºç»å¯¹è·¯å¾„ / Convert relative to absolute URL
                        if not pdf_url.startswith("http"):
                            pdf_url = urljoin(response.url, pdf_url)
                        break
                
                # æ–¹æ³•2: æŸ¥æ‰¾ embed src / Method 2: Find embed src
                if not pdf_url:
                    embed_match = re.search(r'<embed[^>]+src=["\']([^"\']+\.pdf[^"\']*)["\']', html, re.IGNORECASE)
                    if embed_match:
                        pdf_url = embed_match.group(1)
                        if not pdf_url.startswith("http"):
                            pdf_url = urljoin(response.url, pdf_url)
                
                # æ–¹æ³•3: æŸ¥æ‰¾ button æˆ– link ä¸­çš„ PDF URL / Method 3: Find PDF URL in button or link
                if not pdf_url:
                    button_match = re.search(r'<button[^>]+onclick=["\'][^"\']*["\']([^"\']+\.pdf[^"\']*)["\']', html, re.IGNORECASE)
                    if button_match:
                        pdf_url = button_match.group(1)
                        if not pdf_url.startswith("http"):
                            pdf_url = urljoin(response.url, pdf_url)
                
                # æ–¹æ³•4: æŸ¥æ‰¾ç›´æ¥çš„ PDF é“¾æ¥ï¼ˆæ›´å®½æ¾çš„æ¨¡å¼ï¼‰/ Method 4: Find direct PDF link (more flexible pattern)
                if not pdf_url:
                    # æŸ¥æ‰¾æ‰€æœ‰å¯èƒ½çš„ PDF URL / Find all possible PDF URLs
                    pdf_patterns = [
                        r'https?://[^"\'\s<>]+\.pdf[^"\'\s<>]*',
                        r'//[^"\'\s<>]+\.pdf[^"\'\s<>]*',
                        r'/downloads/[^"\'\s<>]+\.pdf',
                    ]
                    for pattern in pdf_patterns:
                        pdf_match = re.search(pattern, html, re.IGNORECASE)
                        if pdf_match:
                            pdf_url = pdf_match.group(0)
                            if pdf_url.startswith("//"):
                                pdf_url = "https:" + pdf_url
                            elif not pdf_url.startswith("http"):
                                pdf_url = urljoin(response.url, pdf_url)
                            break
                
                # æ–¹æ³•5: æŸ¥æ‰¾ Sci-Hub çš„ä¸‹è½½é“¾æ¥ / Method 5: Find Sci-Hub download link
                if not pdf_url:
                    # Sci-Hub å¯èƒ½ä½¿ç”¨ç‰¹å®šçš„ä¸‹è½½è·¯å¾„ / Sci-Hub may use specific download paths
                    download_match = re.search(r'href=["\']([^"\']*download[^"\']*\.pdf[^"\']*)["\']', html, re.IGNORECASE)
                    if download_match:
                        pdf_url = download_match.group(1)
                        if not pdf_url.startswith("http"):
                            pdf_url = urljoin(response.url, pdf_url)
                
                if pdf_url:
                    # ä¸‹è½½çœŸæ­£çš„ PDF / Download actual PDF
                    pdf_response = requests.get(pdf_url, headers=headers, timeout=30, stream=True)
                    if pdf_response.status_code == 200 and (pdf_response.content[:4] == b"%PDF" or "application/pdf" in pdf_response.headers.get("Content-Type", "").lower()):
                        with open(output_path, "wb") as f:
                            for chunk in pdf_response.iter_content(chunk_size=8192):
                                f.write(chunk)
                        stats["success"] += 1
                        print(f"   âœ… æˆåŠŸ: {output_path.name}")
                    else:
                        stats["failed"] += 1
                        print(f"   âŒ PDF ä¸‹è½½å¤±è´¥ / PDF download failed")
                else:
                    stats["failed"] += 1
                    print(f"   âŒ æ— æ³•æ‰¾åˆ° PDF é“¾æ¥ / Cannot find PDF link")
            else:
                stats["failed"] += 1
                print(f"   âŒ æœªçŸ¥å†…å®¹ç±»å‹: {content_type}")
                
        except Exception as e:
            stats["failed"] += 1
            print(f"   âŒ å¤±è´¥: {e}")
            if output_path.exists():
                output_path.unlink()

        # è¯·æ±‚é—´éš” / Request delay
        if i < len(urls):
            time.sleep(1.0)

    # æ‰“å°ç»Ÿè®¡ / Print statistics
    print("\n" + "=" * 60)
    print("ğŸ“Š ä¸‹è½½æ±‡æ€»:")
    print(f"ğŸ“ æ€»è®¡: {stats['total']}")
    print(f"âœ… æˆåŠŸ: {stats['success']}")
    print(f"â­ï¸  è·³è¿‡: {stats['skipped']}")
    print(f"âŒ å¤±è´¥: {stats['failed']}")
    print("=" * 60)
