# python/utils/analyze.py
# External dependencies / å¤–éƒ¨ä¾èµ–
from pathlib import Path
import re


def _parse_wos_records_with_index(text: str):
    """
    è§£æ WoS è®°å½•æ–‡æœ¬ï¼Œè¿”å›å¸¦ç´¢å¼•çš„è®°å½•åˆ—è¡¨
    Parse WoS record text and return a list of records with indices

    Args:
        text (str): WoS æ ¼å¼çš„æ–‡æœ¬å†…å®¹ / WoS formatted text content

    Returns:
        list: åŒ…å« (ç´¢å¼•, è¡Œåˆ—è¡¨) å…ƒç»„çš„è®°å½•åˆ—è¡¨ / List of records as (index, lines) tuples
    """
    records = []
    raw_blocks = text.strip().split("\nER\n")

    for block in raw_blocks:
        block = block.strip()
        if not block:
            continue
        if block == "EF" or (block.startswith("EF") and len(block.split()) == 1):
            continue

        lines = block.splitlines()
        lines.append("ER")
        records.append((len(records) + 1, lines))

    return records


def _extract_doi_from_record(lines):
    """
    ä»è®°å½•è¡Œä¸­æå– DOI
    Extract DOI from record lines

    Args:
        lines (list): è®°å½•çš„è¡Œåˆ—è¡¨ / List of record lines

    Returns:
        str or None: æå–åˆ°çš„ DOIï¼Œå¦‚æœæœªæ‰¾åˆ°åˆ™è¿”å› None / Extracted DOI or None if not found
    """
    for line in lines:
        if line.startswith("DI"):
            parts = line.split(maxsplit=1)
            if len(parts) == 2:
                doi = parts[1].strip()
                # éªŒè¯ DOI æ ¼å¼ / Validate DOI format
                if re.match(r"^10\.\d{4,9}/[^\s]+$", doi):
                    return doi
    return None


def _read_file_text(file_path: Path) -> str | None:
    """
    è¯»å–æ–‡ä»¶å†…å®¹ï¼Œè‡ªåŠ¨å¤„ç†ç¼–ç é—®é¢˜
    Read file content with automatic encoding handling

    Args:
        file_path (Path): æ–‡ä»¶è·¯å¾„ / File path

    Returns:
        str or None: æ–‡ä»¶å†…å®¹ï¼Œè¯»å–å¤±è´¥æ—¶è¿”å› None / File content or None if read fails
    """
    # å°è¯•ä½¿ç”¨ UTF-8-SIG ç¼–ç è¯»å– / Try reading with UTF-8-SIG encoding
    try:
        with open(file_path, "r", encoding="utf-8-sig") as f:
            return f.read()
    except UnicodeDecodeError:
        # å¦‚æœå¤±è´¥ï¼Œå°è¯•ä½¿ç”¨ latin1 ç¼–ç  / If failed, try latin1 encoding
        try:
            with open(file_path, "r", encoding="latin1") as f:
                return f.read()
        except Exception as e:
            print(f"âš ï¸ æ— æ³•è¯»å– {file_path.name}: {e}")
            return None


def _analyze_file(file_path: Path):
    """
    åˆ†æå•ä¸ª WoS txt æ–‡ä»¶ï¼Œè¿”å›ç»Ÿè®¡ä¿¡æ¯
    Analyze a single WoS txt file and return statistics

    Args:
        file_path (Path): æ–‡ä»¶è·¯å¾„ / File path

    Returns:
        dict or None: åŒ…å«ç»Ÿè®¡ä¿¡æ¯çš„å­—å…¸ï¼Œè¯»å–å¤±è´¥æ—¶è¿”å› None
                    Dictionary containing statistics, or None if read fails
    """
    text = _read_file_text(file_path)
    if text is None:
        return None

    records = _parse_wos_records_with_index(text)
    total = len(records)

    valid_dois = []
    missing_records = []

    # éå†æ‰€æœ‰è®°å½•ï¼Œæå– DOI / Iterate through all records to extract DOIs
    for idx, lines in records:
        doi = _extract_doi_from_record(lines)
        if doi:
            valid_dois.append(doi)
        else:
            missing_records.append((idx, "\n".join(lines)))

    return {
        "file": file_path.name,
        "total_records": total,
        "valid_dois": len(valid_dois),
        "missing_count": len(missing_records),
        "missing_details": missing_records,
    }


def doi_checker(archive_dir: Path):
    """
    ä» archive ç›®å½•åŠ è½½æ‰€æœ‰ DOI è®°å½•ï¼Œæ£€æŸ¥ç¼ºå¤±æƒ…å†µ
    Load all DOI records from archive directory and check for missing ones

    Args:
        archive_dir (Path): archive ç›®å½•è·¯å¾„ / Archive directory path
    """
    if not archive_dir.exists():
        print(f"âŒ ç›®å½•ä¸å­˜åœ¨: {archive_dir.resolve()}")
        return

    # è·å–æ‰€æœ‰ .txt æ–‡ä»¶å¹¶æ’åº / Get all .txt files and sort them
    txt_files = sorted([f for f in archive_dir.glob("*.txt")])
    if not txt_files:
        print(f"ğŸ“­ {archive_dir} ä¸‹æ²¡æœ‰ .txt æ–‡ä»¶")
        return

    print(f"ğŸ” å‘ç° {len(txt_files)} ä¸ª .txt æ–‡ä»¶ï¼Œå¼€å§‹æ‰¹é‡åˆ†æ...\n")

    all_stats = []
    grand_total_records = 0
    grand_total_dois = 0
    grand_missing = 0
    all_dois = []  # æ”¶é›†æ‰€æœ‰ DOI ç”¨äºå»é‡ç»Ÿè®¡ / Collect all DOIs for unique count
    doi_file_map = (
        {}
    )  # è¿½è¸ªæ¯ä¸ª DOI å‡ºç°çš„æ–‡ä»¶å’Œæ¬¡æ•° / Track which files each DOI appears in and count

    # å¤„ç†æ¯ä¸ªæ–‡ä»¶ / Process each file
    for file_path in txt_files:
        print(f"ğŸ“„ å¤„ç†æ–‡ä»¶: {file_path.name}")
        stats = _analyze_file(file_path)
        if stats is None:
            continue

        all_stats.append(stats)
        grand_total_records += stats["total_records"]
        grand_total_dois += stats["valid_dois"]
        grand_missing += stats["missing_count"]

        # æ”¶é›†è¯¥æ–‡ä»¶çš„æ‰€æœ‰ DOI å¹¶è®°å½•æ–‡ä»¶ä¿¡æ¯ / Collect all DOIs from this file and track file info
        text = _read_file_text(file_path)
        if text:
            records = _parse_wos_records_with_index(text)
            for idx, lines in records:
                doi = _extract_doi_from_record(lines)
                if doi:
                    all_dois.append(doi)
                    # è®°å½• DOI å‡ºç°çš„æ–‡ä»¶å’Œæ¬¡æ•° / Record which file this DOI appears in and count
                    if doi not in doi_file_map:
                        doi_file_map[doi] = {}
                    # ç»Ÿè®¡æ¯ä¸ªæ–‡ä»¶ä¸­å‡ºç°çš„æ¬¡æ•° / Count occurrences in each file
                    if file_path.name not in doi_file_map[doi]:
                        doi_file_map[doi][file_path.name] = 0
                    doi_file_map[doi][file_path.name] += 1

        # å¦‚æœè¯¥æ–‡ä»¶æœ‰ç¼ºå¤±ï¼Œæ‰“å°è¯¦æƒ… / If this file has missing DOIs, print details
        if stats["missing_count"] > 0:
            print(f"   âŒ {stats['missing_count']} æ¡è®°å½•ç¼ºå¤± DOI")
            for idx, content in stats["missing_details"]:
                print(f"\n   --- {file_path.name} | æ—  DOI è®°å½• #{idx} ---")
                # ç¼©è¿›å†…å®¹ä»¥ä¾¿é˜…è¯» / Indent content for readability
                print(f"   {content.replace(chr(10), chr(10) + '   ')}")
        else:
            print(f"   âœ… å…¨éƒ¨ {stats['total_records']} æ¡è®°å½•å‡æœ‰ DOI")

        print()  # ç©ºè¡Œåˆ†éš” / Empty line separator

    # === æœ€ç»ˆæ±‡æ€» / Final Summary ===
    unique_dois = len(set(all_dois))  # å”¯ä¸€ DOI æ•°é‡ / Unique DOI count
    print("=" * 60)
    print("ğŸ“Š æ‰¹é‡åˆ†ææ±‡æ€»:")
    print(f"ğŸ“ æ–‡ä»¶æ€»æ•°: {len(txt_files)}")
    print(f"ğŸ“š æ€»æ–‡çŒ®è®°å½•æ•°: {grand_total_records}")
    print(f"âœ… æ€»æœ‰æ•ˆ DOI æ•°ï¼ˆå«é‡å¤ï¼‰: {grand_total_dois}")
    print(f"ğŸ”‘ å”¯ä¸€ DOI æ•°: {unique_dois}")
    print(f"âŒ æ€»ç¼ºå¤± DOI æ•°: {grand_missing}")
    if grand_total_records > 0:
        coverage = grand_total_dois / grand_total_records * 100
        print(f"ğŸ“ˆ DOI è¦†ç›–ç‡: {coverage:.2f}%")

    # æ£€æŸ¥å¹¶æ‰“å°é‡å¤ DOI è¯¦æƒ… / Check and print duplicate DOI details
    if grand_total_dois > unique_dois:
        duplicates = grand_total_dois - unique_dois
        print(f"\nğŸ”„ å‘ç° {duplicates} ä¸ªé‡å¤ DOI:")
        # æ‰¾å‡ºæœ‰é‡å¤çš„ DOIï¼ˆè·¨æ–‡ä»¶é‡å¤æˆ–åŒä¸€æ–‡ä»¶å†…é‡å¤ï¼‰/ Find DOIs with duplicates (across files or within same file)
        duplicate_dois = {
            doi: file_counts
            for doi, file_counts in doi_file_map.items()
            if sum(file_counts.values()) > 1  # æ€»å‡ºç°æ¬¡æ•° > 1
        }
        for doi, file_counts in sorted(duplicate_dois.items()):
            total_count = sum(file_counts.values())
            file_list = []
            for filename, count in sorted(file_counts.items()):
                if count > 1:
                    file_list.append(f"{filename} (å‡ºç° {count} æ¬¡)")
                else:
                    file_list.append(filename)
            print(f"   ğŸ“„ {doi}")
            if len(file_counts) > 1:
                print(
                    f"      è·¨ {len(file_counts)} ä¸ªæ–‡ä»¶ï¼Œå…±å‡ºç° {total_count} æ¬¡: {', '.join(file_list)}"
                )
            else:
                print(
                    f"      åœ¨åŒä¸€æ–‡ä»¶ä¸­å‡ºç° {total_count} æ¬¡: {', '.join(file_list)}"
                )

    print("=" * 60)


def doi_extractor(archive_dir: Path) -> list[str]:
    """
    ä» archive ç›®å½•æå–æ‰€æœ‰æœ‰æ•ˆçš„ DOI
    Extract all valid DOIs from archive directory

    Args:
        archive_dir (Path): archive ç›®å½•è·¯å¾„ / Archive directory path

    Returns:
        list[str]: DOI åˆ—è¡¨ï¼ˆå·²å»é‡ï¼‰/ List of unique DOIs
    """
    if not archive_dir.exists():
        return []

    dois = []
    txt_files = sorted([f for f in archive_dir.glob("*.txt")])

    # å¤ç”¨å·²æœ‰çš„è§£æå‡½æ•° / Reuse existing parsing functions
    for file_path in txt_files:
        text = _read_file_text(file_path)
        if text is None:
            continue

        records = _parse_wos_records_with_index(text)
        for idx, lines in records:
            doi = _extract_doi_from_record(lines)
            if doi:
                dois.append(doi)

    unique_dois = list(set(dois))  # å»é‡ / Remove duplicates
    print(f"ğŸ” å‘ç° {len(unique_dois)} ä¸ªæœ‰æ•ˆ DOI")
    return unique_dois
