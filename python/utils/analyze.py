# python/utils/analyze.py
# External dependencies / å¤–éƒ¨ä¾èµ–
from rich.progress import Progress, SpinnerColumn, BarColumn, TextColumn
from rich.console import Console
from rich.table import Table
from rich.panel import Panel
from pathlib import Path
from rich import box
import re


def _parse_wos_records_with_index(text: str):
    """
    è§£æ WoS è®°å½•æ–‡æœ¬ï¼Œè¿”å›å¸¦ç´¢å¼•çš„è®°å½•åˆ—è¡¨
    Parse WoS record text and return a list of records with indices

    Args:
        text (str): WoS æ ¼å¼çš„æ–‡æœ¬å†…å®¹ / WoS formatted text content

    Returns:
        list: åŒ…å« (ç´¢å¼•, è¡Œåˆ—è¡¨) å…ƒç»„çš„è®°å½•åˆ—è¡¨ / List of records as (index, lines) tuples
    """
    records = []
    raw_blocks = text.strip().split("\nER\n")

    for block in raw_blocks:
        block = block.strip()
        if not block:
            continue
        if block == "EF" or (block.startswith("EF") and len(block.split()) == 1):
            continue

        lines = block.splitlines()
        lines.append("ER")
        records.append((len(records) + 1, lines))

    return records


def _extract_doi_from_record(lines):
    """
    ä»è®°å½•è¡Œä¸­æå– DOI
    Extract DOI from record lines

    Args:
        lines (list): è®°å½•çš„è¡Œåˆ—è¡¨ / List of record lines

    Returns:
        str or None: æå–åˆ°çš„ DOIï¼Œå¦‚æœæœªæ‰¾åˆ°åˆ™è¿”å› None / Extracted DOI or None if not found
    """
    for line in lines:
        if line.startswith("DI"):
            parts = line.split(maxsplit=1)
            if len(parts) == 2:
                doi = parts[1].strip()
                # éªŒè¯ DOI æ ¼å¼ / Validate DOI format
                if re.match(r"^10\.\d{4,9}/[^\s]+$", doi):
                    return doi
    return None


def _read_file_text(file_path: Path) -> str | None:
    """
    è¯»å–æ–‡ä»¶å†…å®¹ï¼Œè‡ªåŠ¨å¤„ç†ç¼–ç é—®é¢˜
    Read file content with automatic encoding handling

    Args:
        file_path (Path): æ–‡ä»¶è·¯å¾„ / File path

    Returns:
        str or None: æ–‡ä»¶å†…å®¹ï¼Œè¯»å–å¤±è´¥æ—¶è¿”å› None / File content or None if read fails
    """
    # å°è¯•ä½¿ç”¨ UTF-8-SIG ç¼–ç è¯»å– / Try reading with UTF-8-SIG encoding
    try:
        with open(file_path, "r", encoding="utf-8-sig") as f:
            return f.read()
    except UnicodeDecodeError:
        # å¦‚æœå¤±è´¥ï¼Œå°è¯•ä½¿ç”¨ latin1 ç¼–ç  / If failed, try latin1 encoding
        try:
            with open(file_path, "r", encoding="latin1") as f:
                return f.read()
        except Exception as e:
            print(f"âš ï¸ æ— æ³•è¯»å– {file_path.name}: {e}")
            return None


def _analyze_file(file_path: Path):
    """
    åˆ†æå•ä¸ª WoS txt æ–‡ä»¶ï¼Œè¿”å›ç»Ÿè®¡ä¿¡æ¯
    Analyze a single WoS txt file and return statistics

    Args:
        file_path (Path): æ–‡ä»¶è·¯å¾„ / File path

    Returns:
        dict or None: åŒ…å«ç»Ÿè®¡ä¿¡æ¯çš„å­—å…¸ï¼Œè¯»å–å¤±è´¥æ—¶è¿”å› None
                    Dictionary containing statistics, or None if read fails
    """
    text = _read_file_text(file_path)
    if text is None:
        return None

    records = _parse_wos_records_with_index(text)
    total = len(records)

    valid_dois = []
    missing_records = []

    # éå†æ‰€æœ‰è®°å½•ï¼Œæå– DOI / Iterate through all records to extract DOIs
    for idx, lines in records:
        doi = _extract_doi_from_record(lines)
        if doi:
            valid_dois.append(doi)
        else:
            missing_records.append((idx, "\n".join(lines)))

    return {
        "file": file_path.name,
        "total_records": total,
        "valid_dois": len(valid_dois),
        "missing_count": len(missing_records),
        "missing_details": missing_records,
    }


def doi_checker(archive_dir: Path):
    """
    ä» archive ç›®å½•åŠ è½½æ‰€æœ‰ DOI è®°å½•ï¼Œæ£€æŸ¥ç¼ºå¤±æƒ…å†µ
    Load all DOI records from archive directory and check for missing ones

    Args:
        archive_dir (Path): archive ç›®å½•è·¯å¾„ / Archive directory path
    """
    console = Console()
    
    if not archive_dir.exists():
        console.print(f"[bold red]âŒ ç›®å½•ä¸å­˜åœ¨:[/bold red] {archive_dir.resolve()}")
        return

    # è·å–æ‰€æœ‰ .txt æ–‡ä»¶å¹¶æ’åº / Get all .txt files and sort them
    txt_files = sorted([f for f in archive_dir.glob("*.txt")])
    if not txt_files:
        console.print(f"[yellow]ğŸ“­ {archive_dir} ä¸‹æ²¡æœ‰ .txt æ–‡ä»¶[/yellow]")
        return

    console.print(f"\n[bold cyan]ğŸ” å‘ç°[/bold cyan] [yellow]{len(txt_files)} ä¸ª .txt æ–‡ä»¶[/yellow]ï¼Œ[bold cyan]å¼€å§‹æ‰¹é‡åˆ†æ...[/bold cyan]\n")

    all_stats = []
    grand_total_records = 0
    grand_total_dois = 0
    grand_missing = 0
    all_dois = []  # æ”¶é›†æ‰€æœ‰ DOI ç”¨äºå»é‡ç»Ÿè®¡ / Collect all DOIs for unique count
    doi_file_map = (
        {}
    )  # è¿½è¸ªæ¯ä¸ª DOI å‡ºç°çš„æ–‡ä»¶å’Œæ¬¡æ•° / Track which files each DOI appears in and count

    # å¤„ç†æ¯ä¸ªæ–‡ä»¶ / Process each file
    with Progress(
        SpinnerColumn(),
        TextColumn("[progress.description]{task.description}"),
        BarColumn(),
        TextColumn("[progress.percentage]{task.percentage:>3.0f}%"),
        console=console,
    ) as progress:
        task = progress.add_task("[cyan]ğŸ“„ å¤„ç†æ–‡ä»¶[/cyan]", total=len(txt_files))
        
        for file_path in txt_files:
            stats = _analyze_file(file_path)
            if stats is None:
                progress.update(task, advance=1)
                continue

            all_stats.append(stats)
            grand_total_records += stats["total_records"]
            grand_total_dois += stats["valid_dois"]
            grand_missing += stats["missing_count"]

            # æ”¶é›†è¯¥æ–‡ä»¶çš„æ‰€æœ‰ DOI å¹¶è®°å½•æ–‡ä»¶ä¿¡æ¯ / Collect all DOIs from this file and track file info
            text = _read_file_text(file_path)
            if text:
                records = _parse_wos_records_with_index(text)
                for idx, lines in records:
                    doi = _extract_doi_from_record(lines)
                    if doi:
                        all_dois.append(doi)
                        # è®°å½• DOI å‡ºç°çš„æ–‡ä»¶å’Œæ¬¡æ•° / Record which file this DOI appears in and count
                        if doi not in doi_file_map:
                            doi_file_map[doi] = {}
                        # ç»Ÿè®¡æ¯ä¸ªæ–‡ä»¶ä¸­å‡ºç°çš„æ¬¡æ•° / Count occurrences in each file
                        if file_path.name not in doi_file_map[doi]:
                            doi_file_map[doi][file_path.name] = 0
                        doi_file_map[doi][file_path.name] += 1

            # å¦‚æœè¯¥æ–‡ä»¶æœ‰ç¼ºå¤±ï¼Œæ‰“å°è¯¦æƒ… / If this file has missing DOIs, print details
            if stats["missing_count"] > 0:
                console.print(f"   [red]âŒ {stats['missing_count']} æ¡è®°å½•ç¼ºå¤± DOI[/red]")
                for idx, content in stats["missing_details"]:
                    console.print(Panel(
                        content,
                        title=f"[yellow]{file_path.name}[/yellow] | [red]æ—  DOI è®°å½• #{idx}[/red]",
                        border_style="red",
                        expand=False,
                    ))
            else:
                console.print(f"   [green]âœ… å…¨éƒ¨ {stats['total_records']} æ¡è®°å½•å‡æœ‰ DOI[/green]")

            progress.update(task, advance=1)

    # === æœ€ç»ˆæ±‡æ€» / Final Summary ===
    unique_dois = len(set(all_dois))  # å”¯ä¸€ DOI æ•°é‡ / Unique DOI count
    
    # åˆ›å»ºæ±‡æ€»è¡¨æ ¼ / Create summary table
    summary_table = Table(title="ğŸ“Š æ‰¹é‡åˆ†ææ±‡æ€» / Batch Analysis Summary", box=box.ROUNDED)
    summary_table.add_column("é¡¹ç›® / Item", style="cyan", no_wrap=True)
    summary_table.add_column("æ•°å€¼ / Value", style="magenta", justify="right")
    
    summary_table.add_row("ğŸ“ æ–‡ä»¶æ€»æ•° / Total Files", f"{len(txt_files)}")
    summary_table.add_row("ğŸ“š æ€»æ–‡çŒ®è®°å½•æ•° / Total Records", f"{grand_total_records}")
    summary_table.add_row("âœ… æ€»æœ‰æ•ˆ DOI æ•°ï¼ˆå«é‡å¤ï¼‰/ Total DOIs (with duplicates)", f"{grand_total_dois}")
    summary_table.add_row("ğŸ”‘ å”¯ä¸€ DOI æ•° / Unique DOIs", f"[green]{unique_dois}[/green]")
    summary_table.add_row("âŒ æ€»ç¼ºå¤± DOI æ•° / Missing DOIs", f"[red]{grand_missing}[/red]")
    
    if grand_total_records > 0:
        coverage = grand_total_dois / grand_total_records * 100
        coverage_color = "green" if coverage >= 95 else "yellow" if coverage >= 80 else "red"
        summary_table.add_row("ğŸ“ˆ DOI è¦†ç›–ç‡ / DOI Coverage", f"[{coverage_color}]{coverage:.2f}%[/{coverage_color}]")
    
    console.print()
    console.print(summary_table)

    # æ£€æŸ¥å¹¶æ‰“å°é‡å¤ DOI è¯¦æƒ… / Check and print duplicate DOI details
    if grand_total_dois > unique_dois:
        duplicates = grand_total_dois - unique_dois
        console.print(f"\n[bold yellow]ğŸ”„ å‘ç° {duplicates} ä¸ªé‡å¤ DOI:[/bold yellow]")
        
        # æ‰¾å‡ºæœ‰é‡å¤çš„ DOIï¼ˆè·¨æ–‡ä»¶é‡å¤æˆ–åŒä¸€æ–‡ä»¶å†…é‡å¤ï¼‰/ Find DOIs with duplicates (across files or within same file)
        duplicate_dois = {
            doi: file_counts
            for doi, file_counts in doi_file_map.items()
            if sum(file_counts.values()) > 1  # æ€»å‡ºç°æ¬¡æ•° > 1
        }
        
        # åˆ›å»ºé‡å¤ DOI è¡¨æ ¼ / Create duplicate DOI table
        dup_table = Table(box=box.SIMPLE)
        dup_table.add_column("DOI", style="cyan")
        dup_table.add_column("è¯¦æƒ… / Details", style="yellow")
        
        for doi, file_counts in sorted(duplicate_dois.items()):
            total_count = sum(file_counts.values())
            file_list = []
            for filename, count in sorted(file_counts.items()):
                if count > 1:
                    file_list.append(f"{filename} ([red]å‡ºç° {count} æ¬¡[/red])")
                else:
                    file_list.append(filename)
            
            if len(file_counts) > 1:
                details = f"è·¨ [cyan]{len(file_counts)}[/cyan] ä¸ªæ–‡ä»¶ï¼Œå…±å‡ºç° [red]{total_count}[/red] æ¬¡: {', '.join(file_list)}"
            else:
                details = f"åœ¨åŒä¸€æ–‡ä»¶ä¸­å‡ºç° [red]{total_count}[/red] æ¬¡: {', '.join(file_list)}"
            
            dup_table.add_row(f"[bold]{doi}[/bold]", details)
        
        console.print(dup_table)


def doi_extractor(archive_dir: Path) -> list[str]:
    """
    ä» archive ç›®å½•æå–æ‰€æœ‰æœ‰æ•ˆçš„ DOI
    Extract all valid DOIs from archive directory

    Args:
        archive_dir (Path): archive ç›®å½•è·¯å¾„ / Archive directory path

    Returns:
        list[str]: DOI åˆ—è¡¨ï¼ˆå·²å»é‡ï¼‰/ List of unique DOIs
    """
    console = Console()
    
    if not archive_dir.exists():
        return []

    dois = []
    txt_files = sorted([f for f in archive_dir.glob("*.txt")])

    # å¤ç”¨å·²æœ‰çš„è§£æå‡½æ•° / Reuse existing parsing functions
    for file_path in txt_files:
        text = _read_file_text(file_path)
        if text is None:
            continue

        records = _parse_wos_records_with_index(text)
        for idx, lines in records:
            doi = _extract_doi_from_record(lines)
            if doi:
                dois.append(doi)

    # å»é‡ï¼ˆä¸ä¿æŒé¡ºåºï¼Œæå‡æ€§èƒ½ï¼‰/ Remove duplicates (order not preserved for performance)
    unique_dois = list(set(dois))
    
    console.print(f"[bold cyan]ğŸ” å‘ç°[/bold cyan] [green]{len(unique_dois)} ä¸ªæœ‰æ•ˆ DOI[/green]")
    return unique_dois
