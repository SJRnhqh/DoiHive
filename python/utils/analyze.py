# python/utils/analyze.py
# External dependencies / å¤–éƒ¨ä¾èµ–
from rich.progress import Progress, SpinnerColumn, BarColumn, TextColumn
from rich.console import Console
from rich.table import Table
from rich.panel import Panel
from pathlib import Path
from rich import box
import logging
import re


# Local modules / æœ¬åœ°æ¨¡å—
from .logger import log_to_file_only


def _parse_wos_records_with_index(text: str):
    """
    è§£æ WoS è®°å½•æ–‡æœ¬ï¼Œè¿”å›å¸¦ç´¢å¼•çš„è®°å½•åˆ—è¡¨
    Parse WoS record text and return a list of records with indices

    Args:
        text (str): WoS æ ¼å¼çš„æ–‡æœ¬å†…å®¹ / WoS formatted text content

    Returns:
        list: åŒ…å« (ç´¢å¼•, è¡Œåˆ—è¡¨) å…ƒç»„çš„è®°å½•åˆ—è¡¨ / List of records as (index, lines) tuples
    """
    records = []
    raw_blocks = text.strip().split("\nER\n")

    for block in raw_blocks:
        block = block.strip()
        if not block:
            continue
        if block == "EF" or (block.startswith("EF") and len(block.split()) == 1):
            continue

        lines = block.splitlines()
        lines.append("ER")
        records.append((len(records) + 1, lines))

    return records


def _extract_doi_from_record(lines):
    """
    ä»è®°å½•è¡Œä¸­æå– DOI
    Extract DOI from record lines

    Args:
        lines (list): è®°å½•çš„è¡Œåˆ—è¡¨ / List of record lines

    Returns:
        str or None: æå–åˆ°çš„ DOIï¼Œå¦‚æœæœªæ‰¾åˆ°åˆ™è¿”å› None / Extracted DOI or None if not found
    """
    for line in lines:
        if line.startswith("DI"):
            parts = line.split(maxsplit=1)
            if len(parts) == 2:
                doi = parts[1].strip()
                # éªŒè¯ DOI æ ¼å¼ / Validate DOI format
                if re.match(r"^10\.\d{4,9}/[^\s]+$", doi):
                    return doi
    return None


def _read_file_text(file_path: Path) -> str | None:
    """
    è¯»å–æ–‡ä»¶å†…å®¹ï¼Œè‡ªåŠ¨å¤„ç†ç¼–ç é—®é¢˜
    Read file content with automatic encoding handling

    Args:
        file_path (Path): æ–‡ä»¶è·¯å¾„ / File path

    Returns:
        str or None: æ–‡ä»¶å†…å®¹ï¼Œè¯»å–å¤±è´¥æ—¶è¿”å› None / File content or None if read fails
    """
    # å°è¯•ä½¿ç”¨ UTF-8-SIG ç¼–ç è¯»å– / Try reading with UTF-8-SIG encoding
    try:
        with open(file_path, "r", encoding="utf-8-sig") as f:
            return f.read()
    except UnicodeDecodeError:
        # å¦‚æœå¤±è´¥ï¼Œå°è¯•ä½¿ç”¨ latin1 ç¼–ç  / If failed, try latin1 encoding
        try:
            with open(file_path, "r", encoding="latin1") as f:
                return f.read()
        except Exception as e:
            print(f"âš ï¸ æ— æ³•è¯»å– {file_path.name}: {e}")
            return None


def _analyze_file(file_path: Path):
    """
    åˆ†æå•ä¸ª WoS txt æ–‡ä»¶ï¼Œè¿”å›ç»Ÿè®¡ä¿¡æ¯
    Analyze a single WoS txt file and return statistics

    Args:
        file_path (Path): æ–‡ä»¶è·¯å¾„ / File path

    Returns:
        dict or None: åŒ…å«ç»Ÿè®¡ä¿¡æ¯çš„å­—å…¸ï¼Œè¯»å–å¤±è´¥æ—¶è¿”å› None
                    Dictionary containing statistics, or None if read fails
    """
    text = _read_file_text(file_path)
    if text is None:
        return None

    records = _parse_wos_records_with_index(text)
    total = len(records)

    valid_dois = []
    missing_records = []

    # éå†æ‰€æœ‰è®°å½•ï¼Œæå– DOI / Iterate through all records to extract DOIs
    for idx, lines in records:
        doi = _extract_doi_from_record(lines)
        if doi:
            valid_dois.append(doi)
        else:
            missing_records.append((idx, "\n".join(lines)))

    return {
        "file": file_path.name,
        "total_records": total,
        "valid_dois": len(valid_dois),
        "missing_count": len(missing_records),
        "missing_details": missing_records,
    }


def doi_checker(archive_dir: Path):
    """
    ä» archive ç›®å½•åŠ è½½æ‰€æœ‰ DOI è®°å½•ï¼Œæ£€æŸ¥ç¼ºå¤±æƒ…å†µ
    Load all DOI records from archive directory and check for missing ones

    Args:
        archive_dir (Path): archive ç›®å½•è·¯å¾„ / Archive directory path
    """
    logger = logging.getLogger("doihive")
    console = Console()

    if not archive_dir.exists():
        error_msg = f"âŒ ç›®å½•ä¸å­˜åœ¨: {archive_dir.resolve()}"
        log_to_file_only(logging.ERROR, error_msg)
        console.print(f"[bold red]{error_msg}[/bold red]")
        return

    # è·å–æ‰€æœ‰ .txt æ–‡ä»¶å¹¶æ’åº / Get all .txt files and sort them
    txt_files = sorted([f for f in archive_dir.glob("*.txt")])
    if not txt_files:
        warn_msg = f"ğŸ“­ {archive_dir} ä¸‹æ²¡æœ‰ .txt æ–‡ä»¶"
        log_to_file_only(logging.WARNING, warn_msg)
        console.print(f"[yellow]{warn_msg}[/yellow]")
        return

    info_msg = f"ğŸ” å‘ç° {len(txt_files)} ä¸ª .txt æ–‡ä»¶ï¼Œå¼€å§‹æ‰¹é‡åˆ†æ..."
    log_to_file_only(logging.INFO, info_msg)
    # æ§åˆ¶å°ç¾åŒ–è¾“å‡ºï¼Œä¸é‡å¤æ—¥å¿— / Beautified console output, no duplicate log
    console.print(
        f"\n[bold cyan]ğŸ” å‘ç°[/bold cyan] [yellow]{len(txt_files)} ä¸ª .txt æ–‡ä»¶[/yellow]ï¼Œ[bold cyan]å¼€å§‹æ‰¹é‡åˆ†æ...[/bold cyan]\n"
    )

    all_stats = []
    grand_total_records = 0
    grand_total_dois = 0
    grand_missing = 0
    all_dois = []  # æ”¶é›†æ‰€æœ‰ DOI ç”¨äºå»é‡ç»Ÿè®¡ / Collect all DOIs for unique count
    doi_file_map = (
        {}
    )  # è¿½è¸ªæ¯ä¸ª DOI å‡ºç°çš„æ–‡ä»¶å’Œæ¬¡æ•° / Track which files each DOI appears in and count

    # å¤„ç†æ¯ä¸ªæ–‡ä»¶ / Process each file
    with Progress(
        SpinnerColumn(),
        TextColumn("[progress.description]{task.description}"),
        BarColumn(),
        TextColumn("[progress.percentage]{task.percentage:>3.0f}%"),
        console=console,
    ) as progress:
        task = progress.add_task("[cyan]âœ… å¤„ç†æ–‡ä»¶[/cyan]", total=len(txt_files))

        for file_path in txt_files:
            stats = _analyze_file(file_path)
            if stats is None:
                progress.update(task, advance=1)
                continue

            all_stats.append(stats)
            grand_total_records += stats["total_records"]
            grand_total_dois += stats["valid_dois"]
            grand_missing += stats["missing_count"]

            # æ”¶é›†è¯¥æ–‡ä»¶çš„æ‰€æœ‰ DOI å¹¶è®°å½•æ–‡ä»¶ä¿¡æ¯ / Collect all DOIs from this file and track file info
            text = _read_file_text(file_path)
            if text:
                records = _parse_wos_records_with_index(text)
                for idx, lines in records:
                    doi = _extract_doi_from_record(lines)
                    if doi:
                        all_dois.append(doi)
                        # è®°å½• DOI å‡ºç°çš„æ–‡ä»¶å’Œæ¬¡æ•° / Record which file this DOI appears in and count
                        if doi not in doi_file_map:
                            doi_file_map[doi] = {}
                        # ç»Ÿè®¡æ¯ä¸ªæ–‡ä»¶ä¸­å‡ºç°çš„æ¬¡æ•° / Count occurrences in each file
                        if file_path.name not in doi_file_map[doi]:
                            doi_file_map[doi][file_path.name] = 0
                        doi_file_map[doi][file_path.name] += 1

            # æ›´æ–°è¿›åº¦æ¡ / Update progress bar
            progress.update(task, advance=1)
            
            # æ˜¾ç¤ºè¯¥æ–‡ä»¶çš„åŸºæœ¬ä¿¡æ¯ï¼ˆæ— è®ºæ˜¯å¦æœ‰ç¼ºå¤±ï¼‰/ Display this file's basic info (whether missing or not)
            progress.print()  # ç©ºè¡Œåˆ†éš” / Empty line separator
            
            # æ˜¾ç¤ºæ–‡ä»¶ä¿¡æ¯å’Œç´¯è®¡æ€»æ•° / Display file info and cumulative total
            file_info_msg = f"ğŸ“„ {file_path.name}: {stats['total_records']} æ¡è®°å½• (ç´¯è®¡: {grand_total_records} æ¡)"
            progress.print(f"[cyan]{file_info_msg}[/cyan]")
            
            # å¦‚æœè¯¥æ–‡ä»¶æœ‰ç¼ºå¤±ï¼Œæ‰“å°è¯¦æƒ… / If this file has missing DOIs, print details
            if stats["missing_count"] > 0:
                error_msg = f"   âŒ {stats['missing_count']} æ¡è®°å½•ç¼ºå¤± DOI"
                log_to_file_only(logging.WARNING, error_msg)
                for idx, content in stats["missing_details"]:
                    panel = Panel(
                        content,
                        title=f"[yellow]{file_path.name}[/yellow] | [red]æ—  DOI è®°å½• #{idx}[/red]",
                        border_style="red",
                        expand=False,
                    )
                    # æ ¼å¼åŒ–å¤šè¡Œå†…å®¹ï¼Œæ¯è¡Œæ·»åŠ ç¼©è¿› / Format multi-line content with indentation
                    formatted_content = "\n".join(
                        f"    {line}" for line in content.split("\n")
                    )
                    log_to_file_only(
                        logging.WARNING,
                        f"æ—  DOI è®°å½•: {file_path.name} #{idx}\n{formatted_content}",
                    )
                    progress.print(panel)
            else:
                success_msg = f"   âœ… å…¨éƒ¨ {stats['total_records']} æ¡è®°å½•å‡æœ‰ DOI"
                log_to_file_only(logging.INFO, success_msg)
            
            # ç©ºä¸€è¡Œåˆ†éš” / Empty line separator
            progress.print()

    # === æœ€ç»ˆæ±‡æ€» / Final Summary ===
    unique_dois = len(set(all_dois))  # å”¯ä¸€ DOI æ•°é‡ / Unique DOI count

    # åˆ›å»ºæ±‡æ€»è¡¨æ ¼ / Create summary table
    summary_table = Table(
        title="ğŸ“Š æ‰¹é‡åˆ†ææ±‡æ€» / Batch Analysis Summary", box=box.ROUNDED
    )
    summary_table.add_column("é¡¹ç›® / Item", style="cyan", no_wrap=True)
    summary_table.add_column("æ•°å€¼ / Value", style="magenta", justify="right")

    summary_table.add_row("ğŸ“ æ–‡ä»¶æ€»æ•° / Total Files", f"{len(txt_files)}")
    summary_table.add_row("ğŸ“š æ€»æ–‡çŒ®è®°å½•æ•° / Total Records", f"{grand_total_records}")
    summary_table.add_row(
        "âœ… æ€»æœ‰æ•ˆ DOI æ•°ï¼ˆå«é‡å¤ï¼‰/ Total DOIs (with duplicates)",
        f"{grand_total_dois}",
    )
    summary_table.add_row(
        "ğŸ”‘ å”¯ä¸€ DOI æ•° / Unique DOIs", f"[green]{unique_dois}[/green]"
    )
    summary_table.add_row(
        "âŒ æ€»ç¼ºå¤± DOI æ•° / Missing DOIs", f"[red]{grand_missing}[/red]"
    )

    if grand_total_records > 0:
        coverage = grand_total_dois / grand_total_records * 100
        coverage_color = (
            "green" if coverage >= 95 else "yellow" if coverage >= 80 else "red"
        )
        summary_table.add_row(
            "ğŸ“ˆ DOI è¦†ç›–ç‡ / DOI Coverage",
            f"[{coverage_color}]{coverage:.2f}%[/{coverage_color}]",
        )

    # è®°å½•æ±‡æ€»ä¿¡æ¯åˆ°æ—¥å¿—ï¼ˆåªå†™å…¥æ–‡ä»¶ï¼Œä¸æ˜¾ç¤ºåœ¨æ§åˆ¶å°ï¼Œé¿å…ä¸è¡¨æ ¼é‡å¤ï¼‰ / Log summary to file only (not shown in console to avoid duplication with table)
    log_to_file_only(logging.INFO, "=" * 70)
    log_to_file_only(logging.INFO, "ğŸ“Š æ‰¹é‡åˆ†ææ±‡æ€»:")
    log_to_file_only(logging.INFO, f"ğŸ“ æ–‡ä»¶æ€»æ•°: {len(txt_files)}")
    log_to_file_only(logging.INFO, f"ğŸ“š æ€»æ–‡çŒ®è®°å½•æ•°: {grand_total_records}")
    log_to_file_only(logging.INFO, f"âœ… æ€»æœ‰æ•ˆ DOI æ•°ï¼ˆå«é‡å¤ï¼‰: {grand_total_dois}")
    log_to_file_only(logging.INFO, f"ğŸ”‘ å”¯ä¸€ DOI æ•°: {unique_dois}")
    log_to_file_only(logging.INFO, f"âŒ æ€»ç¼ºå¤± DOI æ•°: {grand_missing}")
    if grand_total_records > 0:
        coverage = grand_total_dois / grand_total_records * 100
        log_to_file_only(logging.INFO, f"ğŸ“ˆ DOI è¦†ç›–ç‡: {coverage:.2f}%")
    log_to_file_only(logging.INFO, "=" * 70)

    # æ§åˆ¶å°åªæ˜¾ç¤ºè¡¨æ ¼ï¼Œä¸æ˜¾ç¤ºæ—¥å¿— / Console only shows table, no log output
    console.print()
    console.print(summary_table)

    # æ£€æŸ¥å¹¶æ‰“å°é‡å¤ DOI è¯¦æƒ… / Check and print duplicate DOI details
    if grand_total_dois > unique_dois:
        duplicates = grand_total_dois - unique_dois
        dup_msg = f"ğŸ”„ å‘ç° {duplicates} ä¸ªé‡å¤ DOI:"
        log_to_file_only(logging.INFO, dup_msg)
        # æ§åˆ¶å°ç¾åŒ–è¾“å‡º / Beautified console output
        console.print(f"\n[bold yellow]{dup_msg}[/bold yellow]")

        # æ‰¾å‡ºæœ‰é‡å¤çš„ DOIï¼ˆè·¨æ–‡ä»¶é‡å¤æˆ–åŒä¸€æ–‡ä»¶å†…é‡å¤ï¼‰/ Find DOIs with duplicates (across files or within same file)
        duplicate_dois = {
            doi: file_counts
            for doi, file_counts in doi_file_map.items()
            if sum(file_counts.values()) > 1  # æ€»å‡ºç°æ¬¡æ•° > 1
        }

        # åˆ›å»ºé‡å¤ DOI è¡¨æ ¼ / Create duplicate DOI table
        dup_table = Table(box=box.SIMPLE)
        dup_table.add_column("DOI", style="cyan")
        dup_table.add_column("è¯¦æƒ… / Details", style="yellow")

        for doi, file_counts in sorted(duplicate_dois.items()):
            total_count = sum(file_counts.values())
            file_list = []
            for filename, count in sorted(file_counts.items()):
                if count > 1:
                    file_list.append(f"{filename} ([red]å‡ºç° {count} æ¬¡[/red])")
                else:
                    file_list.append(filename)

            if len(file_counts) > 1:
                details = f"è·¨ [cyan]{len(file_counts)}[/cyan] ä¸ªæ–‡ä»¶ï¼Œå…±å‡ºç° [red]{total_count}[/red] æ¬¡: {', '.join(file_list)}"
            else:
                details = f"åœ¨åŒä¸€æ–‡ä»¶ä¸­å‡ºç° [red]{total_count}[/red] æ¬¡: {', '.join(file_list)}"

            dup_table.add_row(f"[bold]{doi}[/bold]", details)
            # ç§»é™¤ Rich æ ‡è®°ååªè®°å½•åˆ°æ–‡ä»¶ / Remove Rich markup and log to file only
            clean_details = (
                details.replace("[red]", "")
                .replace("[/red]", "")
                .replace("[cyan]", "")
                .replace("[/cyan]", "")
            )
            log_to_file_only(logging.INFO, f"é‡å¤ DOI: {doi} - {clean_details}")

        # æ§åˆ¶å°åªæ˜¾ç¤ºè¡¨æ ¼ï¼Œä¸æ˜¾ç¤ºæ—¥å¿— / Console only shows table, no log output
        console.print(dup_table)


def doi_extractor(archive_dir: Path) -> list[str]:
    """
    ä» archive ç›®å½•æå–æ‰€æœ‰æœ‰æ•ˆçš„ DOI
    Extract all valid DOIs from archive directory

    Args:
        archive_dir (Path): archive ç›®å½•è·¯å¾„ / Archive directory path

    Returns:
        list[str]: DOI åˆ—è¡¨ï¼ˆå·²å»é‡ï¼‰/ List of unique DOIs
    """
    console = Console()

    if not archive_dir.exists():
        return []

    dois = []
    txt_files = sorted([f for f in archive_dir.glob("*.txt")])

    # å¤ç”¨å·²æœ‰çš„è§£æå‡½æ•° / Reuse existing parsing functions
    for file_path in txt_files:
        text = _read_file_text(file_path)
        if text is None:
            continue

        records = _parse_wos_records_with_index(text)
        for idx, lines in records:
            doi = _extract_doi_from_record(lines)
            if doi:
                dois.append(doi)

    # å»é‡ï¼ˆä¸ä¿æŒé¡ºåºï¼Œæå‡æ€§èƒ½ï¼‰/ Remove duplicates (order not preserved for performance)
    unique_dois = list(set(dois))

    info_msg = f"ğŸ” å‘ç° {len(unique_dois)} ä¸ªæœ‰æ•ˆ DOI"
    logger = logging.getLogger("doihive")
    log_to_file_only(logging.INFO, info_msg)
    # æ§åˆ¶å°ç¾åŒ–è¾“å‡ºï¼Œä¸é‡å¤æ—¥å¿— / Beautified console output, no duplicate log
    console = Console()
    console.print(f"[bold cyan]{info_msg}[/bold cyan]")
    return unique_dois
